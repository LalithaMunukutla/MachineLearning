{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5df5cd",
   "metadata": {},
   "source": [
    "**Main Challenges of Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8490c",
   "metadata": {},
   "source": [
    "**1. Insufficient quantity of training data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951abd47",
   "metadata": {},
   "source": [
    "**2. Non representative Training data** \n",
    "\n",
    "- If the sample data is too small, you will have sampling noise (non representative data as a result of chance) but even very large samples can be non representative if the sampling method is flawed. It is called **Sampling bias**\n",
    "- In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to, whether you use instance based or model based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc1e6e",
   "metadata": {},
   "source": [
    "**3. Poor Quality Data**\n",
    "\n",
    "- If your traing data is full of errors, outliers and noise; it will make it harder for the system to detect the underlying patterns, so is less likely to performe well.\n",
    "- Spend time cleaning up your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a223402",
   "metadata": {},
   "source": [
    "**4. Irrelevant features**\n",
    "\n",
    "- Feature engineering involves: feature selection, feature extraction, creating new features by gathering new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb9912",
   "metadata": {},
   "source": [
    "**5. Overfitting the training data**\n",
    "\n",
    "- Overfitting means model performs well on training data, but does not generalize well.\n",
    "- It happens when the model is too complex relative to the amount and noisiness of training data. Possible solutions are: \n",
    "    \n",
    "    - Simplify model by selecting one with fewer parameters\n",
    "    - gather more training data\n",
    "    - reduce noise in training data \n",
    "\n",
    "- Constraing the model to reduce the risk of overfitting is called **Regularization**\n",
    "- The amount of regularization to apply during learning can be controlled by hyperparameter (parameter of learning algorithm)\n",
    "- Very large value of regularization hyperparamter result in almost flat model but doesnt overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f710fb",
   "metadata": {},
   "source": [
    "**6. Underfitting the training data**\n",
    "\n",
    "- Select more powerful model, with more params\n",
    "- feeding better features to learning algorithm\n",
    "- Reducing the constraints on model (regularization hyperparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f75cac",
   "metadata": {},
   "source": [
    "**7. Testing and Validating**\n",
    "\n",
    "- Split data into training set and test set\n",
    "- Error rate on new cases is called Generalization error or out-of-sample error\n",
    "- If traing error is low but generalization error is high then overfitting\n",
    "- Train models using traing data, select model and hyperparams that perform best on validation set. Run single final test against test set\n",
    "- Use cross-validation to avoid too much wastage on training data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
